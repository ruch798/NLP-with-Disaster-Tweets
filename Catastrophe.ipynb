{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Catastrophe new.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nhBRS_rNBe0I",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "08a0d0bc-fdd5-4bb0-91f0-70f17d62cf24"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "colab_type": "code",
        "id": "vVjmDIvGBc31",
        "colab": {}
      },
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4svThJ0ABc34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "outputId": "93b281f9-5a5d-4d44-be81-7a6e44437213"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import re\n",
        "import string\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from collections import defaultdict\n",
        "from collections import Counter\n",
        "from nltk.corpus import wordnet as wn\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn import model_selection, naive_bayes\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.preprocessing import MaxAbsScaler\n",
        "from scipy import stats\n",
        "from sklearn.metrics import make_scorer, roc_auc_score\n",
        "from scipy.linalg import svd\n",
        "from numpy import diag\n",
        "from scipy.sparse import csr_matrix\n",
        "from gensim.models import Word2Vec, Doc2Vec, FastText\n",
        "from gensim.models.doc2vec import TaggedDocument\n",
        "from gensim.models.phrases import Phraser, Phrases\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from numpy import zeros\n",
        "from sklearn import svm\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding,LSTM,Dense,SpatialDropout1D,Bidirectional\n",
        "from keras.initializers import Constant\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.optimizers import Adam\n",
        "from spellchecker import SpellChecker\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-9192284455ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mspellchecker\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSpellChecker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'wordnet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'spellchecker'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IOymWt1MBc38",
        "colab": {}
      },
      "source": [
        "!pip install pyspellchecker\n",
        "!pip install keras\n",
        "!pip install nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qfCT4jlCClgJ",
        "colab": {}
      },
      "source": [
        "stop=set(stopwords.words('english'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1L35yVsmBc3-",
        "colab": {}
      },
      "source": [
        "np.random.seed(7712)\n",
        "spell = SpellChecker()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "colab_type": "code",
        "id": "9YyORjEDBc4B",
        "colab": {}
      },
      "source": [
        "train_data = pd.read_csv(\"drive/My Drive/data/nlp-getting-started/train.csv\")\n",
        "train_data.head()\n",
        "print(train_data.dtypes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_CmRTrmZBc4D",
        "colab": {}
      },
      "source": [
        "test_data = pd.read_csv(\"drive/My Drive/data/nlp-getting-started/test.csv\")\n",
        "test_data.head()\n",
        "print(train_data.dtypes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FqQ9GzB6Bc4F",
        "colab": {}
      },
      "source": [
        "train_data = train_data.drop(columns=['id','keyword','location'])\n",
        "test_data = test_data.drop(columns=['id','keyword','location'])\n",
        "train_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pszs64DRBc4I",
        "colab": {}
      },
      "source": [
        "test_data['text']=test_data['text'].replace(\"\",\"empty\")\n",
        "print(train_data.size)\n",
        "print(test_data.size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuP8MsTLnzES",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.concat([train_data,test_data])\n",
        "df.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63Zs1WhSn8nv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_URL(text):\n",
        "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    return url.sub(r'',str(text))\n",
        "\n",
        "def remove_html(text):\n",
        "    html=re.compile(r'<.*?>')\n",
        "    return html.sub(r'',str(text))\n",
        "\n",
        "def remove_emoji(text):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', text)\n",
        "\n",
        "def remove_punct(text):\n",
        "    table=str.maketrans('','',string.punctuation)\n",
        "    return text.translate(table)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqFmkAapn-RM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['text'] = df['text'].apply(lambda a : remove_URL(a))\n",
        "df['text'] = df['text'].apply(lambda a : remove_html(a))\n",
        "df['text'] = df['text'].apply(lambda a : remove_emoji(a))\n",
        "df['text'] = df['text'].apply(lambda a : remove_punct(a))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64IR_AS3oF87",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = df[:train_data.shape[0]]\n",
        "test_data = df[train_data.shape[0]:]\n",
        "train_data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "J3vTqbSqBc4L",
        "colab": {}
      },
      "source": [
        "y=1\n",
        "def correct_spellings(text):\n",
        "    global y\n",
        "    print(y)\n",
        "    y = y+1\n",
        "    corrected_text = []\n",
        "    misspelled_words = spell.unknown(text.split())\n",
        "    for word in text.split():\n",
        "        if (\"@\" in word) or (\"#\" in word) or (\"http\" in word):\n",
        "            corrected_text.append(word)\n",
        "        elif word in misspelled_words:\n",
        "            corrected_text.append(spell.correction(word))\n",
        "        else:\n",
        "            corrected_text.append(word)\n",
        "    return \" \".join(corrected_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "k6XN_axKBc4N",
        "colab": {}
      },
      "source": [
        "train_data['text']=train_data['text'].apply(lambda x : correct_spellings(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPX1x8UJyPQ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data.to_csv(r'drive/My Drive/data/nlp-getting-started/spellcheck_output_train2.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jnG2Q4DzBc4P",
        "colab": {}
      },
      "source": [
        "test_data['text']=test_data['text'].apply(lambda x : correct_spellings(x))\n",
        "# test_data=pd.read_csv(\"spellcheck_output_test.csv\")\n",
        "# test_data = test_data.drop(['Unnamed: 0'], axis=1)\n",
        "# test_data.head()\n",
        "# test_data['text'] = test_data['text'].apply(lambda x: correct_spellings(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PyRkITnh38V7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data.to_csv(r'drive/My Drive/data/nlp-getting-started/spellcheck_output_test2.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KQ64GAlDBc4R",
        "colab": {}
      },
      "source": [
        "# pd.options.display.max_columns = None\n",
        "# pd.options.display.max_rows = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "obD1yzuDBc4X",
        "colab": {}
      },
      "source": [
        "count = train_data['target'].value_counts()\n",
        "count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jxzuLvVeBc4Z",
        "colab": {}
      },
      "source": [
        "def plot_bar(df, feat_x, feat_y, normalize=True):\n",
        "    \"\"\" Plot with vertical bars of the requested dataframe and features\"\"\"\n",
        "    \n",
        "    ct = pd.crosstab(df[feat_x], df[feat_y])\n",
        "    if normalize == True:\n",
        "        ct = ct.div(ct.sum(axis=1), axis=0)\n",
        "    return ct.plot(kind='bar', stacked=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HHZ5jhIgBc4c",
        "colab": {}
      },
      "source": [
        "pd.value_counts(train_data['target']).plot.bar()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pF4zT45RBc4e",
        "colab": {}
      },
      "source": [
        "splits = StratifiedShuffleSplit(n_splits=1,test_size=0.3,random_state=69)\n",
        "for train_i, test_i in splits.split(train_data,train_data['target']):\n",
        "    train_set = train_data.loc[train_i]\n",
        "    test_set = train_data.loc[test_i]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4x8qTm8-M25r",
        "colab": {}
      },
      "source": [
        "# train_set=train_set.reset_index(drop=True)\n",
        "test_set.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Rx9CFL2KBc4i",
        "colab": {}
      },
      "source": [
        "# train_set = process(train_set)\n",
        "# test_set = process(test_set)\n",
        "\n",
        "frames = [train_set,test_set]\n",
        "train_data = pd.concat(frames)\n",
        "\n",
        "train_data = train_data.reset_index(drop=True)\n",
        "# test_data = process(test_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "INTGR_xqBc4k",
        "colab": {}
      },
      "source": [
        "train_set.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CRccJVgvBc4m",
        "colab": {}
      },
      "source": [
        "def create_corpus(train_data):\n",
        "    corpus=[]\n",
        "    for tweet in train_data['text']:\n",
        "        words=[word.lower() for word in word_tokenize(tweet) if((word.isalpha()==1) & (word not in stop))]\n",
        "        corpus.append(words)\n",
        "    return corpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "43A5LRekBc4n",
        "colab": {}
      },
      "source": [
        "corpus=create_corpus(train_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "e2yeeT12Bc4p",
        "colab": {}
      },
      "source": [
        "embedding_dict={}\n",
        "with open('glove.6B.100d.txt','r') as f:\n",
        "    for line in f:\n",
        "        values=line.split()\n",
        "        word=values[0]\n",
        "        vectors=np.asarray(values[1:],'float32')\n",
        "        embedding_dict[word]=vectors\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Nt3V0it9Bc4r",
        "colab": {}
      },
      "source": [
        "MAX_LEN=50\n",
        "tokenizer_obj=Tokenizer()\n",
        "tokenizer_obj.fit_on_texts(corpus)\n",
        "sequences=tokenizer_obj.texts_to_sequences(corpus)\n",
        "\n",
        "text_padding=pad_sequences(sequences,maxlen=MAX_LEN,truncating='post',padding='post')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LOPt47WJBc4t",
        "colab": {}
      },
      "source": [
        "word_index=tokenizer_obj.word_index\n",
        "print('Number of unique words:',len(word_index))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZdIz35Z8Bc4w",
        "colab": {}
      },
      "source": [
        "num_words=len(word_index)+1\n",
        "embedding_matrix=np.zeros((num_words,100))\n",
        "\n",
        "for word,i in word_index.items():\n",
        "    if i > num_words:\n",
        "        continue\n",
        "    \n",
        "    emb_vec=embedding_dict.get(word)\n",
        "    if emb_vec is not None:\n",
        "        embedding_matrix[i]=emb_vec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VUxXyCZmBc4x",
        "colab": {}
      },
      "source": [
        "model=Sequential()\n",
        "n_timesteps = 10\n",
        "embedding=Embedding(num_words,100,embeddings_initializer=Constant(embedding_matrix),\n",
        "                   input_length=MAX_LEN,trainable=False)\n",
        "\n",
        "model.add(embedding)\n",
        "model.add(SpatialDropout1D(0.2))\n",
        "model.add(Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2), input_shape=(n_timesteps, 1)))\n",
        "# model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "\n",
        "optimzer=Adam(learning_rate=1e-5)\n",
        "\n",
        "model.compile(loss='binary_crossentropy',optimizer=optimzer,metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "84IG74w7Bc4z",
        "colab": {}
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "idnKyOTOBc40",
        "colab": {}
      },
      "source": [
        "train=text_padding[:train_data.shape[0]]\n",
        "test=text_padding[train_data.shape[0]:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5bjh-peqBc42",
        "colab": {}
      },
      "source": [
        "X_train,X_test,y_train,y_test=train_test_split(train,train_data['target'].values,test_size=0.15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7HefCjWpBc44",
        "colab": {}
      },
      "source": [
        "history=model.fit(X_train,y_train,batch_size=4,epochs=15,validation_data=(X_test,y_test),verbose=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4m_ktNxUnNpm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test1=text_padding[:test_data.shape[0]]\n",
        "test2=text_padding[test_data.shape[0]:]\n",
        "predictions=history.model.predict_classes(x=test1,batch_size=None,verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dy4tQ1DLnNpn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "output_df = pd.DataFrame(predictions,columns=['target'])\n",
        "output_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gFqs2OrLBc47",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "model = AdaBoostClassifier(n_estimators=100, random_state = 1)\n",
        "model.fit(X_train, y_train)\n",
        "test_pred = model.predict(X_test)\n",
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_score(y_test, test_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2rxWNhZCBc49",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "Rando= RandomForestClassifier(n_estimators=100)\n",
        "Rando.fit(X_train,y_train)\n",
        "y_pred1=Rando.predict(X_test)\n",
        "accuracy_score(y_test,y_pred1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5D05UDX9Bc4_",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "logreg=LogisticRegression(C=1e5)\n",
        "logreg.fit(X_train,y_train)\n",
        "y_pred2=logreg.predict(X_test)\n",
        "accuracy_score(y_test,y_pred2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "29aoNsvTBc5B",
        "colab": {}
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "svc = DecisionTreeClassifier().fit(X_train,y_train)\n",
        "y_pred2 = svc.predict(X_test)\n",
        "accuracy_score(y_test,y_pred2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ifg7tq4PKN-_",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}